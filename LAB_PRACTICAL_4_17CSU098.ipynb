{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LAB PRACTICAL 4_17CSU098.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUfYFIWyFOLC"
      },
      "source": [
        "#**TEXT AND WEB ANALYTICS**\r\n",
        "\r\n",
        "##**LAB PRACTICAL 4**\r\n",
        "\r\n",
        "###**MAHIMA MUNJAL** \r\n",
        "\r\n",
        "###**17CSU098**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV7YlQW74uR8"
      },
      "source": [
        "**Q1. Demonstrate the computation of Similarity Metrics such as Jaccard, Levenshtein and Cosine.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeHgWwhU5PHB"
      },
      "source": [
        "**JACCARD SIMILARITY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K__kRL595Krw"
      },
      "source": [
        "#Jaccard Similarity - Method 1\r\n",
        "\r\n",
        "def jaccard_similarity(list1, list2):\r\n",
        "    intersection = len(list(set(list1).intersection(list2)))\r\n",
        "    union = (len(list1) + len(list2)) - intersection\r\n",
        "    return float(intersection)/union"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIq-aRYa5OqI",
        "outputId": "95ba9258-6e4f-4670-e10f-9c2f3e0f4780"
      },
      "source": [
        "data1=input()\r\n",
        "data2=input()\r\n",
        "list1 = data1.split(\" \")\r\n",
        "list2 = data2.split(\" \")\r\n",
        "print(\"List 1 \",list1)\r\n",
        "print(\"List 2 \",list2)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mahima Munjal is a good girl\n",
            "Mahima Munjal studies at NorthCap University\n",
            "List 1  ['Mahima', 'Munjal', 'is', 'a', 'good', 'girl']\n",
            "List 2  ['Mahima', 'Munjal', 'studies', 'at', 'NorthCap', 'University']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kGfqAkx5lnI",
        "outputId": "f01b11ea-0cfb-45f4-b426-4661c2582541"
      },
      "source": [
        "jaccard_similarity(list1, list2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yT1IVqn5oZC"
      },
      "source": [
        "#Jaccard Similarity - Method 2\r\n",
        "\r\n",
        "def jaccard_similarities(list1, list2):\r\n",
        "    s1 = set(list1)\r\n",
        "    s2 = set(list2)\r\n",
        "    return float(len(s1.intersection(s2)) / len(s1.union(s2)))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvUdMCyu5uqv",
        "outputId": "4af55704-1328-4dbf-c7ba-959ff58c2c6a"
      },
      "source": [
        "jaccard_similarities(list1, list2)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkE7GE7QrT4C",
        "outputId": "85c66f42-13bb-4e05-ca49-da06812a8e7a"
      },
      "source": [
        "#Jaccard Similarity -Method 3\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from sklearn.metrics import jaccard_score\r\n",
        "\r\n",
        "jaccard_score([1,1,0,0],[0,1,0,1])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKdYSrBnqGPh",
        "outputId": "e726600d-93ea-4159-cd87-c23834c3baa4"
      },
      "source": [
        "pip install jaccard-index "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jaccard-index\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/66/a066229192ef1323b5a36bfc68a7d2e850227f96c0754349072369470255/jaccard_index-0.0.3-py3-none-any.whl\n",
            "Installing collected packages: jaccard-index\n",
            "Successfully installed jaccard-index-0.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv1Jarv4qiwA",
        "outputId": "e3b5f493-fc16-4ca2-b0d7-c2f8f7f8f63a"
      },
      "source": [
        "#Jaccard Index \r\n",
        "\r\n",
        "rom jaccard_index.jaccard import jaccard_index\r\n",
        "jaccard_index(\"Mahima\",\"Mahima Munjal\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMFbAFOo53Jd"
      },
      "source": [
        "**LEVENSHTEIN DISTANCE**\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pui1iGxT56hK",
        "outputId": "ad59d263-03f1-4ceb-cbf7-6c267f8ee043"
      },
      "source": [
        "!apt install -qq enchant\r\n",
        "!pip install pyenchant"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "0 upgraded, 10 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 1,310 kB of archives.\n",
            "After this operation, 5,353 kB of additional disk space will be used.\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-5build6) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n",
            "Unpacking emacsen-common (2.0.8) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.27.2) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../4-aspell_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n",
            "Unpacking aspell-en (2017.08.24-0-0.1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2017.08.24) ...\n",
            "Selecting previously unselected package libhunspell-1.6-0:amd64.\n",
            "Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Selecting previously unselected package libenchant1c2a:amd64.\n",
            "Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Selecting previously unselected package enchant.\n",
            "Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking enchant (1.6.0-11.1) ...\n",
            "Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up emacsen-common (2.0.8) ...\n",
            "Setting up libtext-iconv-perl (1.7-5build6) ...\n",
            "Setting up dictionaries-common (1.27.2) ...\n",
            "Setting up aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up hunspell-en-us (1:2017.08.24) ...\n",
            "Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Setting up aspell-en (2017.08.24-0-0.1) ...\n",
            "Setting up enchant (1.6.0-11.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n",
            "Collecting pyenchant\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/8c/bd224a5db562ac008edbfaf015f5d5c98ea13e745247cd4ab5fc5b683085/pyenchant-3.2.0-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.5MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbuweSvW1LqC"
      },
      "source": [
        "**LEVENSHTEIN DISTANCE**\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0Py8sRP5_MX",
        "outputId": "ddac84c5-0bd0-4c9f-d9d5-ade1ddec53ba"
      },
      "source": [
        "#Levenshtein Distance\r\n",
        "\r\n",
        "import enchant\r\n",
        "\r\n",
        "data1 = \"Mahima Munjal is a good girl.\"\r\n",
        "data2 = \"Mahima Munjal\"\r\n",
        "\r\n",
        "enchant.utils.levenshtein(data1,data2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQyigqQf6GD4"
      },
      "source": [
        "**COSINE SIMILARITY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMKgU7zR6EFB",
        "outputId": "3d1d49ae-2030-4e57-9818-611bf10e0f1f"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLssd0Ds6Mrv"
      },
      "source": [
        "from nltk.corpus import stopwords \r\n",
        "from nltk.tokenize import word_tokenize "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc0Fprlr6P0u",
        "outputId": "b1f20a31-1bfd-4776-9517-a0b556d0817d"
      },
      "source": [
        "I = input(\"Enter first string: \").lower() \r\n",
        "II = input(\"Enter second string: \").lower() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter first string: Mahima Munjal is a good girl\n",
            "Enter second string: Mahima Munjal studies at NCU and works at Nagarro\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAnL1SES6Z5Z",
        "outputId": "972b51b8-e747-45ab-966a-2e5ce7cad44f"
      },
      "source": [
        "# tokenization \r\n",
        "I_list = word_tokenize(I)  \r\n",
        "II_list = word_tokenize(II) \r\n",
        "print('First List', I_list)\r\n",
        "print('Second List', II_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First List ['mahima', 'munjal', 'is', 'a', 'good', 'girl']\n",
            "Second List ['mahima', 'munjal', 'studies', 'at', 'ncu', 'and', 'works', 'at', 'nagarro']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFUhzhr-6vUA",
        "outputId": "b8b1535f-3c8c-4ab5-d36f-e1ea90a5d726"
      },
      "source": [
        "#removing stopwords\r\n",
        "\r\n",
        "sw = stopwords.words('english')  \r\n",
        "l1 =[];l2 =[] \r\n",
        "I_set = {w for w in I_list if not w in sw}  \r\n",
        "II_set = {w for w in II_list if not w in sw} \r\n",
        "print('First Set', I_set)\r\n",
        "print('Second Set', II_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Set {'good', 'mahima', 'girl', 'munjal'}\n",
            "Second Set {'nagarro', 'ncu', 'studies', 'munjal', 'works', 'mahima'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi6eesuj7KIx"
      },
      "source": [
        "# Forming a set containing keywords of both strings \r\n",
        "\r\n",
        "rvector = I_set.union(II_set)  \r\n",
        "for w in rvector: \r\n",
        "    if w in I_set: l1.append(1) \r\n",
        "    else: l1.append(0) \r\n",
        "    if w in II_set: l2.append(1) \r\n",
        "    else: l2.append(0) \r\n",
        "c = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKa854TS7TqV",
        "outputId": "daa383a1-5a69-426b-dba0-43c88a5389e5"
      },
      "source": [
        "# cosine formula  \r\n",
        "for i in range(len(rvector)): \r\n",
        "        c+= l1[i]*l2[i] \r\n",
        "cosine = c / float((sum(l1)*sum(l2))**0.5) \r\n",
        "\r\n",
        "print('Cosine Similarity Value :',cosine)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cosine Similarity Value : 0.4082482904638631\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flYpaMFU8uDZ"
      },
      "source": [
        "**COSINE SIMILARITY MATRIX**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qgCJyOu7pxk",
        "outputId": "8e428903-af89-4972-d02f-55b479a9a67f"
      },
      "source": [
        "I = input(\"Enter first string: \").lower() \r\n",
        "II = input(\"Enter second string: \").lower() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter first string: Mahima Munjal is a student of Text and Web Analytics\n",
            "Enter second string: Mahima Munjal is a student of The Northcap University\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmykVSJ08_lb"
      },
      "source": [
        "documents = [I,II]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyDunXQH9FCg"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "import pandas as pd"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "K2LtJDKI9IKU",
        "outputId": "aae6192c-d169-4ba5-9e78-1f916a135a65"
      },
      "source": [
        "count_vectorizer = CountVectorizer(stop_words='english')\r\n",
        "count_vectorizer = CountVectorizer()\r\n",
        "sparse_matrix = count_vectorizer.fit_transform(documents)\r\n",
        "\r\n",
        "\r\n",
        "doc_term_matrix = sparse_matrix.todense()\r\n",
        "df = pd.DataFrame(doc_term_matrix, \r\n",
        "                  columns=count_vectorizer.get_feature_names(), \r\n",
        "                  index=['I', 'II'])\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>analytics</th>\n",
              "      <th>and</th>\n",
              "      <th>is</th>\n",
              "      <th>mahima</th>\n",
              "      <th>munjal</th>\n",
              "      <th>northcap</th>\n",
              "      <th>of</th>\n",
              "      <th>student</th>\n",
              "      <th>text</th>\n",
              "      <th>the</th>\n",
              "      <th>university</th>\n",
              "      <th>web</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>I</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>II</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    analytics  and  is  mahima  munjal  ...  student  text  the  university  web\n",
              "I           1    1   1       1       1  ...        1     1    0           0    1\n",
              "II          0    0   1       1       1  ...        1     0    1           1    0\n",
              "\n",
              "[2 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Slhgfzjv9N6Z",
        "outputId": "d347d20a-e618-4242-85aa-17627d231318"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "print(cosine_similarity(df, df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.58925565]\n",
            " [0.58925565 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkaCqvQF9bPu"
      },
      "source": [
        "**Q2. CALCULATE THE TFIDF VECTORIZOR ON 2 DOCUMENTS.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_xJURl_91_N",
        "outputId": "eaf7aa39-2efe-49fa-b2b8-7d92b056e741"
      },
      "source": [
        "I = input(\"Enter first string: \").lower() \r\n",
        "II = input(\"Enter second string: \").lower() "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter first string: Mahima Munjal loves to watch movies\n",
            "Enter second string: Mahima Munjal loves to do yoga\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcSu6amn9TG-",
        "outputId": "e14c8ef8-a084-4662-c9d3-403a94c15602"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "corpus = [I,II]\r\n",
        "vectorizer = TfidfVectorizer()\r\n",
        "X = vectorizer.fit_transform(corpus)\r\n",
        "print('Vectorizer Features :',vectorizer.get_feature_names())\r\n",
        "print('Vectorizer Shape: ',X.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorizer Features : ['do', 'loves', 'mahima', 'movies', 'munjal', 'to', 'watch', 'yoga']\n",
            "Vectorizer Shape:  (2, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jIeFMuluJDw",
        "outputId": "c1450ee8-b748-4003-9cef-d6128108f956"
      },
      "source": [
        "print(\"TF-IDF Scores\")\r\n",
        "print(X)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF-IDF Scores\n",
            "  (0, 3)\t0.49844627974580596\n",
            "  (0, 6)\t0.49844627974580596\n",
            "  (0, 5)\t0.35464863330313684\n",
            "  (0, 1)\t0.35464863330313684\n",
            "  (0, 4)\t0.35464863330313684\n",
            "  (0, 2)\t0.35464863330313684\n",
            "  (1, 7)\t0.49844627974580596\n",
            "  (1, 0)\t0.49844627974580596\n",
            "  (1, 5)\t0.35464863330313684\n",
            "  (1, 1)\t0.35464863330313684\n",
            "  (1, 4)\t0.35464863330313684\n",
            "  (1, 2)\t0.35464863330313684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NOlxNTRAhyY"
      },
      "source": [
        "**Q3. Apply the max-df, min-df param in the TF-IDF function.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puPuE6E8u-Uq"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OfzwAKUAGpl",
        "outputId": "69ce1ff2-95f2-4cd1-8334-523418f2f6de"
      },
      "source": [
        "data = [I,II]\r\n",
        "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word',ngram_range=(1, 1), max_df=0.50, min_df=1, max_features=None)\r\n",
        "\r\n",
        "count_train = count_vec.fit(data)\r\n",
        "bag_of_words = count_vec.transform(data)\r\n",
        "\r\n",
        "print('Features :',count_vec.get_feature_names())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features : ['movies', 'watch', 'yoga']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1sKXucbuwKG",
        "outputId": "a0da921a-c8f7-4038-ea50-6fe868a5c5bf"
      },
      "source": [
        "print(\"Count Vectorizer Scores\")\r\n",
        "print(bag_of_words)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count Vectorizer Scores\n",
            "  (0, 0)\t1\n",
            "  (0, 1)\t1\n",
            "  (1, 2)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bSz1jYnlwSj"
      },
      "source": [
        "**Q4. Compute Cosine Similarity using both TF-IDF and Count vectorizer.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBw2kO4B5Uve",
        "outputId": "b94ef859-87df-4385-e91e-312b9e9cab78"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import numpy as np\r\n",
        "import numpy.linalg as LA\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_RBxLJG5XAQ"
      },
      "source": [
        "train_set = [\"Mahima Munjal is a good girl\", \"Mahima Munjal studies at The Northcap University.\"]    # Documents\r\n",
        "test_set = [\"A good girl named Mahima Munjal studies at the Northcap University.\"]    # Query\r\n",
        "stopWords = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VemtmpR15Yir",
        "outputId": "27d3103f-58c2-4d9e-9930-756ab909e6b1"
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words = stopWords)\r\n",
        "print('Vectorizer : ',vectorizer)\r\n",
        "transformer = TfidfTransformer()\r\n",
        "print('\\n\\nTF-IDF Transformer : ',transformer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorizer :  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None,\n",
            "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
            "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
            "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
            "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
            "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
            "                            'itself', ...],\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "\n",
            "\n",
            "TF-IDF Transformer :  TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr4QYamf6EO2",
        "outputId": "4e74a3bc-a0cd-4b16-c0e2-9dda6076e57d"
      },
      "source": [
        "#Using Count Vectorizer\r\n",
        "\r\n",
        "trainVectorizerArray = vectorizer.fit_transform(train_set).toarray()\r\n",
        "testVectorizerArray = vectorizer.transform(test_set).toarray()\r\n",
        "print ('Fit Vectorizer to train set \\n', trainVectorizerArray)\r\n",
        "print ('\\n\\nTransform Vectorizer to test set\\n', testVectorizerArray)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fit Vectorizer to train set \n",
            " [[1 1 1 1 0 0 0]\n",
            " [0 0 1 1 1 1 1]]\n",
            "\n",
            "\n",
            "Transform Vectorizer to test set\n",
            " [[1 1 1 1 1 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzebFb9L5hhf",
        "outputId": "e53a6067-0e6c-4a56-dcb0-6aeb9c5a64ae"
      },
      "source": [
        "#Using TF-IDF\r\n",
        "\r\n",
        "transformer.fit(trainVectorizerArray)\r\n",
        "print('Fit transformer to train set \\n',transformer.transform(trainVectorizerArray).toarray())\r\n",
        "transformer.fit(testVectorizerArray)\r\n",
        "tfidf = transformer.transform(testVectorizerArray)\r\n",
        "print('\\n\\nFit transformer to test set\\n',tfidf.todense())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fit transformer to train set \n",
            " [[0.57615236 0.57615236 0.40993715 0.40993715 0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.35520009 0.35520009 0.49922133 0.49922133\n",
            "  0.49922133]]\n",
            "\n",
            "\n",
            "Fit transformer to test set\n",
            " [[0.37796447 0.37796447 0.37796447 0.37796447 0.37796447 0.37796447\n",
            "  0.37796447]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kimZIwYyCpiS"
      },
      "source": [
        "**Q5. Find the words in the vocabulary with the highest TF-IDF score.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OZkUQjREL5-",
        "outputId": "8ab511bf-829b-4579-93ec-0e09dfe853d8"
      },
      "source": [
        "I = input(\"Enter first string: \").lower() \r\n",
        "II = input(\"Enter second string: \").lower() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter first string: Katrina Kaif is the best actress. She is the most versatile actress of current genration. She loves to be the best. She is very sincere and hardworking.\n",
            "Enter second string: Indira Gandhi is the best prime minister till date, She is an epitome of grace, power, intellect and bravery. She is my idol. She was the best in each task she did.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsREcEtUCxG8"
      },
      "source": [
        "import math\r\n",
        "from textblob import TextBlob as tb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmJqorXQDgRl"
      },
      "source": [
        "def tf(word, computation):\r\n",
        "    return computation.words.count(word) / len(computation.words)\r\n",
        "\r\n",
        "def n_containing(word, mylist):\r\n",
        "    return sum(1 for computation in mylist if word in computation.words)\r\n",
        "\r\n",
        "def idf(word, mylist):\r\n",
        "    return math.log(len(mylist) / (1 + n_containing(word, mylist)))\r\n",
        "\r\n",
        "def tfidf(word, computation, mylist):\r\n",
        "    return tf(word, computation) * idf(word, mylist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k5a-OdHDCJ8",
        "outputId": "85b4ad4a-5a6f-4d39-dda7-6530c8cb1e29"
      },
      "source": [
        "I=tb(I)\r\n",
        "II=tb(II)\r\n",
        "mylist = [I,II]\r\n",
        "for i, computation in enumerate(mylist):\r\n",
        "    print(\"\\nTop words in document {}\".format(i + 1))\r\n",
        "    scores = {word: tfidf(word, computation, mylist) for word in computation.words}\r\n",
        "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\r\n",
        "    for word, score in sorted_words[:3]:\r\n",
        "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top words in document 1\n",
            "\tWord: katrina, TF-IDF: 0.0\n",
            "\tWord: kaif, TF-IDF: 0.0\n",
            "\tWord: actress, TF-IDF: 0.0\n",
            "Top words in document 2\n",
            "\tWord: indira, TF-IDF: 0.0\n",
            "\tWord: gandhi, TF-IDF: 0.0\n",
            "\tWord: prime, TF-IDF: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}